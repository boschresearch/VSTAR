{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c36c5c-0857-49e3-a450-ae9e7e413d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02518db5-80d8-495c-b638-38e46edd8327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob, yaml, math, random, json\n",
    "sys.path.append('.')\n",
    "sys.path.append('./scripts/evaluation/')\n",
    "\n",
    "import datetime, time\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from collections import OrderedDict\n",
    "from tqdm import trange, tqdm\n",
    "from einops import rearrange, repeat\n",
    "from functools import partial\n",
    "import torch\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from funcs import load_model_checkpoint, load_prompts, load_image_batch, get_filelist, save_videos\n",
    "from funcs import batch_ddim_sampling\n",
    "from utils.utils import instantiate_from_config, encode_attribute_multiple\n",
    "\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "# My utils\n",
    "from utils.attention_utils import *\n",
    "from utils.vis_utils import *\n",
    "from utils.test_list import regularization_dict, collected_prompt_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59a1c7-2a52-44e9-9bd6-abac18ff8f0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Base T2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ea48faf-e0ad-4a5a-99f8-fb67d7441845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ckpt_path': 'checkpoint/model.ckpt', 'config': 'configs/inference_t2v_512_v2.0.yaml', 'mode': 'base', 'fps': 28, 'width': 512, 'height': 320, 'n_samples': 1, 'bs': 1, 'ddim_steps': 25, 'ddim_eta': 1.0, 'unconditional_guidance_scale': 12, 'savedir': 'results', 'frames': 16, 'savefps': 8}\n"
     ]
    }
   ],
   "source": [
    "ddim_steps = 25\n",
    "unconditional_guidance_scale = 12\n",
    "config = 'configs/inference_t2v_512_v2.0.yaml'\n",
    "ckpt = 'checkpoint/model.ckpt'\n",
    "savedir = 'results'\n",
    "fps = 28\n",
    "height, width = 320, 512\n",
    "gpu_num = 1\n",
    "mode = \"base\"\n",
    "n_samples = 1\n",
    "bs = 1\n",
    "savefps = 8 \n",
    "frames = 16 # Change the number of frames here\n",
    "args_dict = {\n",
    "    \"ckpt_path\": ckpt,\n",
    "    \"config\": config,\n",
    "    \"mode\": mode,\n",
    "    \"fps\": fps,\n",
    "    \"width\": width,\n",
    "    \"height\": height,\n",
    "    \"n_samples\": n_samples,\n",
    "    \"bs\": bs,\n",
    "    \"ddim_steps\": ddim_steps, \"ddim_eta\": 1.0,\n",
    "    \"unconditional_guidance_scale\": unconditional_guidance_scale,\n",
    "    \"savedir\": savedir, \"frames\": frames, \n",
    "    \"savefps\": savefps,\n",
    "}\n",
    "\n",
    "args = OmegaConf.create(args_dict)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a425a-d9dd-4d57-b25c-749093af82b3",
   "metadata": {},
   "source": [
    "# 1: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debe36f6-fa7b-4d5e-8de9-1bd493cce345",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE working on z of shape (1, 4, 64, 64) = 16384 dimensions.\n",
      ">>> model checkpoint loaded.\n",
      "Frames:  16\n"
     ]
    }
   ],
   "source": [
    "## step 1: model config\n",
    "## -----------------------------------------------------------------\n",
    "config = OmegaConf.load(args.config)\n",
    "model_config = config.pop(\"model\", OmegaConf.create())\n",
    "model = instantiate_from_config(model_config)\n",
    "model = model.cuda()\n",
    "assert os.path.exists(args.ckpt_path), f\"Error: checkpoint [{args.ckpt_path}] Not Found!\"\n",
    "model = load_model_checkpoint(model, args.ckpt_path)\n",
    "model.eval()\n",
    "\n",
    "## sample shape\n",
    "assert (args.height % 16 == 0) and (args.width % 16 == 0), \"Error: image size [h,w] should be multiples of 16!\"\n",
    "## latent noise shape\n",
    "h, w = args.height // 8, args.width // 8\n",
    "frames = model.temporal_length if args.frames < 0 else args.frames\n",
    "channels = model.channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ac2ccfb-6694-4467-9eb4-bb84b2d40a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create save_dir:  results/2024-07-29-base_vstar\n"
     ]
    }
   ],
   "source": [
    "# Change here!\n",
    "#postfix = ''\n",
    "postfix = '_vstar'\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "## saving folders\n",
    "save_dir = os.path.join(args.savedir, now + f'-{mode}' + postfix)\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(\"Create save_dir: \", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08c995-ba19-499b-92fc-d3acbf3106c8",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e915f19a-ce3b-4eb7-b1f6-d07706366f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_callback(pred_x0, i):\n",
    "    video = model.decode_first_stage_2DAE(pred_x0).clip(-1.0, 1.0)\n",
    "    video = (video / 2 + 0.5).clamp(0, 1) # -1,1 -> 0,1\n",
    "    save_path_inter =  f\"step{i}.jpg\"\n",
    "    save_path_inter = os.path.join(save_dir_latest,save_path_inter)\n",
    "    save_image_grid(video, save_path_inter, rescale=False, n_rows=8,)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5857b9-616a-44c1-af66-71794cd03769",
   "metadata": {},
   "source": [
    "# 2. Prepare Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64788425-3758-4708-8d4f-66c1c13ef1d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Active Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d506720-52db-4ded-b9b1-6ab47caef980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 A young boy becomes an old man, realistic,best quality, 4k\n",
      "1 A day at the beach from dawn till dusk,eye-level shot\n",
      "2 A Ferrari driving on the road, starts to snow\n",
      "3 The seasonal cycle of a lake from frozen winter to autumn,a shot at eye level\n",
      "4 A day at the beach from dawn till dusk\n",
      "5 A young boy becomes an old man, and turns into a young girl, neon-lit urban landscape background,dystopian cyberpunk,futuristic design, electronic artwork,moody lighting, ultra-detailed,high-tech implants\n",
      "6 Superman flying in the sky, sunny day becomes a dark rainy day, best quality, 4k, realistic\n",
      "7 Spider-Man standing on the beach from morning to evening\n",
      "8 A peony starts to bloom, in the field\n",
      "9 Rainbow appears after the rainy day\n",
      "10 A flower starts to bloom\n",
      "11 A pizza is being made\n",
      "12 A mural being painted on a city wall\n",
      "13 A landscape transitioning from winter to spring\n",
      "14 A young girl is aging\n",
      "15 The sun rises from the sea, making the dark sky bright\n",
      "16 A night sky changing from dusk till dawn\n",
      "17 From sunrise to sunset at the beach\n",
      "18 A makeup transformation\n",
      "19 The seasonal cycle of a lake from frozen winter to summer\n",
      "20 A female person's hairstyle changing through the years\n",
      "21 A tree's transformation through carving into a sculpture\n",
      "22 A camel in the desert from sunrise to sunset till a dark evening\n"
     ]
    }
   ],
   "source": [
    "for i, p_dict in enumerate(collected_prompt_list):\n",
    "    print(i, p_dict[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d60ec736-f01a-428c-acfd-4547c836d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose prompt\n",
    "prompt_id = 0\n",
    "prompt_dict = collected_prompt_list[prompt_id]\n",
    "prompt_list = [\n",
    "    prompt_dict[\"prompt\"]\n",
    "]\n",
    "attribute_list = prompt_dict[\"subprompts\"] # None, if disabling VSP\n",
    "\n",
    "# Set seeds\n",
    "seed_list = [128] \n",
    "\n",
    "# Set number of frames\n",
    "frames = 32\n",
    "\n",
    "cond_image_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e33a3e6-781e-4235-bd40-5ed4adbff333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep attention maps at:  []\n",
      "Total attention layers:  34\n",
      "Use delta attention:  True\n",
      "res64-std1-until13\n"
     ]
    }
   ],
   "source": [
    "# VSTAR Setup\n",
    "\n",
    "keep_timestep_list = []\n",
    "save_timestep_list = [*range(1,26)]\n",
    "save_maps = False # True for saving visualization\n",
    "save_npy = False\n",
    "\n",
    "attention_store = AttentionStore(\n",
    "    base_width=64, base_height=40,\n",
    "    keep_timestep_list=keep_timestep_list,\n",
    "    save_timestep_list=save_timestep_list,\n",
    "    save_maps=save_maps, save_npy=save_npy\n",
    ")\n",
    "use_delta_attention = True\n",
    "\n",
    "# Stardard deviation is a hyperparameter controlling the dynamics\n",
    "# Smaller values has stronger effect\n",
    "# For short videos: \"64D4\", \"64D1\", \"64D8\"\n",
    "# For longer videos, regualarization at res32 becomes necessary: \"64D1_32D8\"\n",
    "setup_key = \"64D1\"\n",
    "ablation_dict = regularization_dict[setup_key]\n",
    "if use_delta_attention:\n",
    "    register_attention_control_vstar(model, attention_store, ablation_dict)\n",
    "else:\n",
    "    register_attention_control(model, attention_store)\n",
    "\n",
    "print(\"Use delta attention: \", use_delta_attention)\n",
    "if use_delta_attention and ablation_dict is None:\n",
    "    print(\"diag_std: \", diag_std)\n",
    "\n",
    "post_fix_folder = \"\"\n",
    "for i,k in enumerate(ablation_dict[\"regularize_res_list\"]):\n",
    "    diag = ablation_dict[f'diag_{k}']\n",
    "    scale = ablation_dict[f'scale_{k}']\n",
    "    if i!=0:\n",
    "        post_fix_folder += '_'\n",
    "    post_fix_folder += f\"res{k}-std{diag}\"\n",
    "    if scale != 1.0:\n",
    "        post_fix_folder += f\"-scale{scale}\"\n",
    "        \n",
    "until_time = ablation_dict[\"until_time\"] \n",
    "post_fix_folder += f\"-until{until_time}\"\n",
    "    \n",
    "if use_delta_attention:\n",
    "    print(post_fix_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e9991f-580c-4b61-93cc-726807da2bdd",
   "metadata": {},
   "source": [
    "## Run Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e899ed-c8d3-4f02-89b2-63ca36671ecc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_list = None\n",
    "interpolation_mode = \"linear\" #\"linear\"\n",
    "\n",
    "## step 3: run over samples\n",
    "## -----------------------------------------------------------------\n",
    "start = time.time()\n",
    "n_rounds = len(seed_list)\n",
    "\n",
    "for idx in range(0, n_rounds):\n",
    "    attention_store.reset()\n",
    "    cur_prompt = prompt_list[0]\n",
    "    cur_seed = seed_list[idx]\n",
    "    seed_everything(cur_seed)\n",
    "    save_prompt = \"-\".join((cur_prompt.replace(\"/\", \"\").split(\" \")[:10]))\n",
    "    save_dir_cur = f\"{save_prompt}-{cur_seed}\"\n",
    "    num_attribute = len(attribute_list)\n",
    "    if num_attribute > 0:\n",
    "        save_dir_cur += f\"_embed{num_attribute}\"\n",
    "    if interpolation_mode != \"linear\":\n",
    "        save_dir_cur += f\"_{interpolation_mode}\"\n",
    "    save_dir_latest = os.path.join(save_dir, save_dir_cur)\n",
    "\n",
    "    if use_delta_attention:\n",
    "        save_dir_latest += f\"_deltaAttn_f{frames}_{post_fix_folder}\" \n",
    "    else:\n",
    "        save_dir_latest += f\"_f{frames}\"\n",
    "    attention_store.set_save_dir(os.path.join(save_dir_latest, \"attention\"))\n",
    "    \n",
    "    print(f'Work on prompt {idx + 1} / {n_rounds}... Seed={cur_seed}')\n",
    "    print(cur_prompt)\n",
    "    batch_size = args.bs\n",
    "    noise_shape = [batch_size, channels, frames, h, w]\n",
    "    fps = torch.tensor([args.fps]*batch_size).to(model.device).long()\n",
    "   \n",
    "    g_cpu = torch.Generator(device=model.device)\n",
    "    g_cpu.manual_seed(cur_seed)\n",
    "    x_T = torch.randn(noise_shape, device=model.device, generator=g_cpu)\n",
    "    \n",
    "    x_T = None\n",
    "    print(f'----> saved in {save_dir_latest}')\n",
    "    \n",
    "    if isinstance(cur_prompt, str):\n",
    "        prompts = [cur_prompt]\n",
    "        \n",
    "    if len(attribute_list) == 0:\n",
    "        print(\"Use normal prompt embedding.\")\n",
    "        text_emb = model.get_learned_conditioning(prompts) # (1,77,1024)\n",
    "    else:\n",
    "        print(\"Use attrbites embeddings.\")\n",
    "        text_emb = encode_attribute_multiple(model, attribute_list, frames,interpolation_mode,indices_list=indices_list)\n",
    "\n",
    "\n",
    "    cond = {\"c_crossattn\": [text_emb], \"fps\": fps}\n",
    "   \n",
    "\n",
    "    ## inference\n",
    "    batch_samples = batch_ddim_sampling(\n",
    "        model, cond, noise_shape, args.n_samples, \n",
    "        args.ddim_steps, args.ddim_eta, args.unconditional_guidance_scale,\n",
    "        verbose=True, img_callback=img_callback,\n",
    "        x_T=x_T,\n",
    "    )\n",
    "    \n",
    "    ## b,samples,c,t,h,w\n",
    "    file_names = [save_dir_cur]\n",
    "    save_videos(batch_samples, save_dir_latest, file_names, fps=args.savefps,ext_name=\"gif\")\n",
    "    final_frame_save_dir = os.path.join(save_dir_latest, 'final_video')\n",
    "    save_image_batch(batch_samples[0,0], final_frame_save_dir, ext_type=\"jpg\")\n",
    "    \n",
    "    # Save config\n",
    "    config_cur = {\n",
    "        \"seed\": cur_seed,\n",
    "        \"prompt\": cur_prompt,\n",
    "        \"attribute_list\": attribute_list,\n",
    "        \"ablation_dict\":ablation_dict,\n",
    "    }\n",
    "    with open(os.path.join(save_dir_latest, f\"{save_dir_cur}.json\"), \"w\") as outfile: \n",
    "        json.dump(config_cur, outfile, indent=4)\n",
    "    print()\n",
    "    \n",
    "print(f\"Saved in {args.savedir}. Time used: {(time.time() - start):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063cb05-07c6-44cc-976d-37eca8060882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5254e61-11e2-4bf3-9abe-6e7bd267ac1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba249c-8f35-481d-a8ec-39f98647685d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vstar]",
   "language": "python",
   "name": "conda-env-vstar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
